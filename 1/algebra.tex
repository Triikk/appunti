\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts} % \mathbb
% \usepackage{centernot}

\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Ima}{Im}

\title{
	Appunti di Algebra \\
	\large A.A. 2022/2023
}
\date{}

\begin{document}

\maketitle

\section{Introduzione}

\subsection{Relazioni}
Una \textbf{relazione} é un sottoinsieme del prodotto cartesiano di due o piú insiemi.\\
Una relazione su $A$ é un sottoinsieme di $A \times A$. \\\\
$a_{1}$ é in relazione con $a_{2}$ e si scrive $a_{1} R a_{2}$. \\
Def. Una relazione é di \textbf{equivalenza} se rispetta le seguenti proprietá:
\begin{itemize}
	\item[] Riflessiva: $a R a$ $\forall a \in A$ (ogni elemento é in relazione con se stesso)
	\item[] Simmetrica: $a_{1} R a_{2} \implies a_{2} R a_{1}$ $\forall a_{1}, a_{2} \in A$
	\item[] Transitiva: $a_1 R a_2 \wedge a_2 R a_3 \implies a_1 R a_3$
\end{itemize}

\subsection{Funzioni/Applicazioni}
$f : X \rightarrow Y$\\\\
$f$ iniettiva: $\forall x_1, x_2 \in X, f(x_1) = f(x_2) \implies x_1=x_2$ \\
$f$ suriettiva: $\forall y \in Y, \exists x \in A : y = f(x)$ \\
$f$ biettiva: $\forall y \in Y, \exists! x \in A : y = f(x)$

\subsection{Insiemi numerici}
L'insieme dei numeri razionali $\mathbb{Q}$ introduce gli inversi del prodotto (es. $3 \rightarrow \frac{1}{3}$). \\
L'insieme dei numeri reali $\mathbb{R}$ introduce limiti, radici e altri valori. \\
L'insieme dei numeri complessi $\mathbb{C}$ introduce le radici di indice pari di numeri negativi tramite l'unitá immaginaria $i$ e i suoi multipli. Un numero complesso é esprimibile in forma polare come $a+ib$, con $a,b \in R$.

\subsection{Campi}
$(K, +, \cdot)$ é un campo se:
\begin{itemize}
	\item[] $+, \cdot$ sono associative ($a+(b+c) = (a+b)+c$), commutative ($a+b=b+a$) e distributive ($a(b+c)=ab+ac$)
	\item[] esistono elementi \textbf{neutri} ($0$ per la somma ($a+0=a$), $1$ per il prodotto ($a \cdot 1 = a$)) e \textbf{opposti} ($-a$ per la somma ($a-a=0$), $x^{-1}$ per il prodotto ($x \cdot x^{-1} = 1$)), che restituscono il valore neutro
\end{itemize}
Alcuni insiemi campi sono $\mathbb{Q}, \mathbb{R}, \mathbb{C}$.

\subsubsection{Campi finiti}
Dato un numero intero $n \ge 0$, definiamo su $\mathbb{Z}$ la relazione di equivalenza
$$a \equiv b (n) \iff \exists k \in Z : a - b = k \cdot n$$
essa rispetta tutte e 3 le proprietá elencate sopra. \\\\
Definiamo $[b] = \{a \in Z : a \equiv b (n)\}$ e $Z_n = \{[0], [1], ..., [n-1]\}$. \\
Es. in $Z_2 = \{[0], [1]\}$, $[0]$ sono i numeri pari, $[1]$ quelli dispari. \\\\
Definiamo su $Z_n$ le operazioni:\\
$$[a] + [b] = [a+b], [a] \cdot [b] = [a \cdot b]$$ \\
Es. Possiamo scrivere, con la notazione dei campi finiti, il prodotto tra numeri interi: \\
Dato $Z_2$: $[0] \cdot [0] = [0], [0] \cdot [1] = [0 \cdot 1] = [0], [1] \cdot [1] = [1 \cdot 1] = [1]$. \\\\
$Z_n$ é un campo $\iff n$ é \textbf{primo}. Se $n$ non é primo, non esisterá l'inverso di un fattore di $n$, ovvero non esisterá nessuna classe di elementi che se moltiplicata con la classe del fattore restituisca classe 1.

\section{Spazi vettoriali}
Uno \textbf{spazio vettoriale} definito su un campo $K$ é un insieme $V$ con due operazioni:
\begin{itemize}
	\item[] $+ : V \times V \rightarrow V$ ($v_1, v_2) \rightarrow v_1 + v_2$
	\item[] $\cdot : K \times V \rightarrow V$ ($a, v \rightarrow av$)
\end{itemize}
che verificano le seguenti proprietá: $+$ é commutativa, associativa, con elem. neutri (vettore nullo) e opposti ($-v$), $\cdot$ é associativa, distribuitiva rispetto alla somma e con elemento neutro.\\\\
Per ogni campo $K$, $K^n$ é uno spazio vettoriale su $K$.\\
$K^n = \{(x_1, x_2, ..., x_n), x_i \in K, \forall i=1, ..., n\} \\
v = (x_1, x_2, ..., x_n), u = (y_1, y_2, ..., y_n) \\
v + u = (x_1 + y_1, x_2 + y_2, ..., x_n + y_n) \\
av = (ax_1, ax_2, ..., ax_n), a \in K
$

\subsection{Sottospazi vettoriali}
Un sottoinsieme non vuoto (contenente almeno il vettore nullo) $U \subseteq V$ (spazio vettoriale su $K$) é un \textbf{sottospazio vettoriale} (SSV) di V se é \textbf{chiuso} rispetto alle sue operazioni, cioé:
\begin{itemize}
	\item $\forall v_1, v_2 \in U \rightarrow v_1 + v_2 \in U$
	\item $\forall v_1 \in U, a \in K, a \cdot v_1 \in U$
\end{itemize}
Esempio: $V = R^2$ spazio vettoriale su R, $U = \{(x,y) \in R^2 : y = 2x\}$. É un SSV?\\
Se $v_1, v_2 \in U: v_1 = (x_1, y_1) \rightarrow y_1 = 2x_1, v_2 = (x_2, y_2) \rightarrow y_2 = 2x_2$\\
$v_1 + v_2 = (x_1 + x_2, 2x_1 + 2x_2) \rightarrow (x_1 + x_2, 2(x_1 + x_2)) \implies v_1 + v_2 \in U$. Inoltre, $\forall a \in R, a \cdot v_1 = a(x_1, 2ax_1) \implies a \cdot v_1 \in U$. Quindi, $U$ é un SSV di $V$.\\
Graficamente, significa che la somma di qualsiasi coppia di vettori presenti sulla retta $y=2x$ é un vettore sempre giacente su questa retta, cosí come il prodotto di qualsiasi vettore giacente sulla retta per un qualsiasi scalare é un vettore sempre giacente su questa retta.\\\\
$U$ é un SSV di $V \iff \forall u_1, u_2 \in U, \forall a_1, a_2 \in K$.\\
Dimostrazione:
\begin{itemize}
	\item[$\Rightarrow$]: se $U$ é un SSV di V e $u_1, u_2 \in U \implies a_1 u_1, a_2 u_2 \in U \implies a_1 u_1 + a_2 u_2$
	\item[$\Leftarrow$]: se $a_1 u_1 + a_2 u_2 \in U \forall a_1, a_2 \in K$, in particolare: prendendo $a_1 = 1, a_2 = 1, u_1 + u_2 \in U$, prendendo $a_1$ qualsiasi e $a_2 = 0, a_1 u_1 \in U$
\end{itemize}

\subsection{Combinazione lineare}
Dati $v_1, v_2, \dots, v_n \in V$, diciamo che $v \in V$ é una \textbf{combinazione lineare} di $v_1, v_2, \dots, v_n$ se $\exists a_1, a_2, \dots, a_n \in K : v = a_1 v_1 + a_2 v_2 + \dots + a_n v_n$, quindi se $v$ é esprimibile come la somma di tutti i vettori di $V$ moltiplicati per un corrispettivo scalare.\\
Per quanto osservato sopra, $U$ é un SSV $\iff$ contiene \textit{tutte le combinazioni lineari} di \textit{tutti i suoi elementi}. Non esiste una combinazione lineare $u$ degli elementi di $U$ che non sia $\in U$.\\\\
Esempio: $V = R^2, v_1 = (1,0), v_2 = (0,1), v_3 = (3, -2)$. $v_3 = 3v_1 -2v_2$ quindi $v_3$ é combinazione lineare di $v_1, v_2$. Altro esempio: $u_1 = (2,0), u_2 = (-1, 0), u_3 = (3, -2)$. $u$ in questo caso non é combinazione lineare di $u_1, u_2$ perché non é possibile ottenere la seconda coordinata -2 essendo 0 in entrambi.

\subsection{Span}
Un SSV $U$ di $V$ é \textbf{generato} da $\{v_1, \dots, v_n\}$ se ogni elemento $u \in U$ é combinazione lineare di $v_1, \dots, v_n$, cioé se $\forall u \in U, \exists a_1, \dots, a_n \in K : u = a_1 v_1 + \dots + a_n v_n$. $U$ é lo \textbf{span} di $\{v_1, \dots, v_n\}$ ed é scritto $U = \langle v_1, \dots, v_n \rangle$. Si noti che l'insieme contiene infiniti elementi, siccome infinite sono le combinazioni lineari ottenibili ($a_1, \dots, a_n \in R$).\\\\
Esempio: $
V = R^4 = \{(x,y,z,w), x,y,z,w \in R\}, \\
v_1 = (2,0,0,0), v_2 = (0,1,-1,0)
$. Il sottospazio generato da $v_1, v_2$ é $\langle v_1, v_2 \rangle = \{a_1 v_1 + a_2 v_2, a_1, a_2 \in R\} = (2a_1,0,0,0) + (0,a_2,-a_2,0) = (2a_1,a_2,-a_2,0) = \{(x,y,z,w) \in R^4 : y + z = 0, w = 0\}$.\\
Altro esempio: $V = R[x], \langle x^2, x, 1 \rangle = \{ax^2+bx+c, a,b,c \in R\} = \{$ tutti i polinomi di grado $\leq 2\}$ (con $a=0$ il grado é $< 2$).

\subsection{Indipendenza lineare}
Un insieme di vettori $\{v_1, \dots, v_n\}$ é \textbf{linearmente indipendente} se \textit{nessun vettore} é la combinazione lineare degli altri vettori dell'insieme, ovvero se l'\textit{unica combinazione lineare di} $v_1, \dots, v_n$ che restituisce il vettore nullo é quella con tutti i coefficienti $a_1, \dots, a_n \in K = \vec{0}$. Questo perché se un vettore é combinazione lineare di un insieme di vettori (es. $v_3 = 2v_1 + 4v_2$), basta dare i giusti coefficienti ($a_1=2, a_2=4, a3=-1)$ per fare in modo che si annullino e mettere i coefficienti degli altri vettori a 0.\\\\
Un insieme di vettori é \textbf{linearmente dipendente} se non é linearmente indipendente. Non é peró detto che ogni vettore apparentenente a un insieme linearmente dipendente sia combinazione lineare di altri (es. $v_1 = (1,0), v_2 = (2,0), v_3 = (0,1) \rightarrow v_2=2v_1$ ma $v_3$ non é combinazione lineare di $v_1, v_2$), é sufficiente che una coppia di vettori sia ricavabile l'una dall'altra per rendere tutto l'insieme linearmente dipendente.

\subsubsection{Equivalenza delle definizioni di indipendenza lineare}
\begin{enumerate}
	\item Nessun vettore tra $v_1, \dots, v_n$ é combinazione lineare degli altri
	\item Se $a_1v_1 + \dots + a_nv_n = 0 \implies a_1=0, \dots,a_n=0$
\end{enumerate}
Se la 1 é falsa, $\exists v_i$ (supponiamo per semplicitá sia $v_1$) che é combinazione lineare degli altri, quindi $v_1 = a_2v_2 + \dots + a_nv_n \iff v_1 - a_2v_2 - \dots - a_nv_n = 0 \implies$ la 2 é anch'essa falsa perché i coefficienti non sono per forza tutti 0 (sicuramente $a_1=1$).\\
Se la 2 é falsa significa che $\exists a_1, \dots, a_n$ con almeno un $a_i \ne 0 : a_1v_1 + \dots + a_nv_n = 0$, allora $\displaystyle v_i = \frac{a_1}{a_i}v_1 + \dots + \frac{a_n}{a_i}v_n \implies$ la 1 é anch'essa falsa siccome $v_i$ é combinazione lineare degli altri.

\section{Base}
Sia $V$ uno spazio vettoriale su $K$, $\{v_1, \dots, v_n\}$ é una \textbf{base} di $V$ se $\{v_1, \dots, v_n\}$ é \textbf{indipendente} e \textbf{genera V}.\\
Ad esempio, per $V=R^2, \{(1,0),(0,1)\}$ é indipendente e genera $R^2$, quindi é una base, mentre $\{(1,0), (0,1), (1,1)\}$ genera $R^2$ ma é lineramente dipendente, quindi non é una base.

\subsection{Teorema. Ogni vettore dello spazio vettoriale si scrive in modo univoco come combinazione lineare dei vettori di una base}
Teorema. $B$ é una base di uno spazio vettoriale $V$ su un campo $K$ $\iff$ $\forall v \in V, \exists! a_1, \dots, a_n \in K : v = a_1v_1 + \dots a_nv_n$ (ogni vettore dello spazio si scrive in modo univoco come combinazione lineare degli altri).\\
In questo caso, $a_1, \dots, a_n$ sono detti le \textbf{coordinate} di $v$ nella base $B$.\\\\
Dimostrazione.
\begin{itemize}
	\item 1. $\implies$ 2\\
Sia $B$ una base, $v \in V$. Poiché $B$ genera $V$ (per ipotesi é una base), $\exists a_1, \dots, a_n : v = a_1v_1 + \dots + a_nv_n$. Per mostrare l'unicitá dei coefficienti, supponiamo $\exists b_1, \dots, b_n : v= b_1v_1 + \dots + b_nv_n$.\\
\begin{equation}
	\begin{cases}
		v = a_1v_1 + \dots + a_nv_n \\
		v = b_1v_1 + \dots + b_nv_n
	\end{cases}
	\begin{cases}
		v - v = (a_1v_1 + \dots + a_nv_n) - (b_1v_1 + \dots + b_nv_n) \\
		0 = v_1 (a_1 - b_1)v_1 + \dots + (a_n - b_n)v_n
	\end{cases}
\end{equation}
Poiché $B$ é linearmente indipendente $\implies a_1-b_1 = \dots = a_n - b_n = 0$, cioé $a_1 = b_1, \dots, a_n = b_n$
	\item 2 $\implies$ 1\\
Per ipotesi, $\forall v \in V, \exists! a_1, \dots, a_n \in K : v = a_1v_1 + \dots + a_nv_n$. Da questo, possiamo dedurre che $B$ genera $V$. Inoltre, sapendo che il vettore nullo é \underline{sempre} ottenibile come combinazione lineare in cui tutti i coefficienti $a_1, \dots, a_n \in K$ sono uguali a 0, sfruttando la loro unicitá, ció implica che $0v_1 + \dots + 0v_n = \vec{0}$, ovvero che $B$ é linearmente indipendente. $B$ genera $V$ ed é linearmente indipendente $\implies$ $B$ é una base.
\end{itemize}

\subsection{Base canonica}
Sia $K^n$ spazio vettoriale di dimensione $n$ del campo $K$. Si definisce l'insieme di vettori $e_1 = (1,0,0, \dots, 0), e_2 = (0,1,0, \dots, 0), \dots, e_n = (0,0,0,\dots,1)$ \textbf{base canonica} di $K^n$.
In generale, é un insieme di vettori $e_1, \dots, e_n$ dove l'$i$-esimo vettore ha la $i$-esima componente a 1 e tutte le altre a 0.\\
Ad esempio, la base canonica di $R^3$ é $\{e_1 = (1,0,0), e_2 = (0,1,0), e_3 = (0,0,1)\}$.\\
La base canonica per l'insieme dei polinomi di grado $\le k$ é $\{1, x, x^2, \dots, x^k\}$.

\subsection{Estrazione e completamento a una base}
Sia $X$ un insieme di vettori che \underline{genera $V$}: \textbf{estrarre} una base da $X$ significa trovare $B \subseteq X$ che sia una base di $V$.\\
Ad esempio, $X = \{v_1 = (1,0), v_2 = (1,1), v_3 = (0,1)\}$, $B = \{v_1, v_3\}$ é una base estratta da $X$.\\\\
Sia $X$ un insieme \underline{linearmente indipendente} in $V$: \textbf{completare} $X$ ad una base di $V$ significa trovare un insieme di vettori da aggiungere ad $X$ in modo da ottenere una base.\\
Ad esempio, $X = \{v_1 = (1,0,0), v_2 = (2,0,0)\}$. $X$ non genera $R^3$ perché $\langle X \rangle = \{(x,y,z) \in R^3 : z = 0$. Per completare $X$ a una base di $R^3$ basta aggiungere un vettore linearmente indipendente dagli altri due e che abbia $z \ne 0$, ad esempio aggiungendo $v_3 = (0,0,1)$ si ottiene la base canonica.\\\\
In generale: sia $V$ spazio vettoriale di dimensione $d$ su un campo $K$:
\begin{itemize}
	\item ogni insieme linearmente indipendente in $V$ contiene $k$ elementi, $k \le d$; puó essere completato a una base di $V$ aggiungendo $d-k$ elementi in modo opportuno (senza rendere l'insieme linearmente dipendente)
	\item ogni insieme che genera $V$ contiene $g$ elementi, $g \ge d$; possiamo estrarre una base rimuovendo opportunamente $g - d$ elementi
\end{itemize}

\subsection{Teorema. Numero di elementi di una base}
Teorema. Tutte le basi di uno spazio vettoriale $V$ su un campo $K$ hanno lo \textbf{stesso numero di elementi} e tale numero é detto la \textbf{dimensione} di $V$. La dimensione puó anche essere pensata come il numero di direzioni linearmente indipendenti sufficienti per potersi muovere in tutto lo spazio vettoriale.\\
Ad esempio, $V = K^n$ ha base canonica $\{e_1, \dots, e_n\}$ composta da $n$ elementi, quindi \underline{qualunque base di $K^n$ ha $n$ elementi} ($dim(K^n) = n$).\\\\
Ad esempio, $V = R[x] = \{\mbox{polinomi}\}$ ha una base $\{1,x,x^2,\dots\} \implies dim(R[x]) = \infty$, mentre $U_n = \{p(x) \in R[x] : p(x)$ ha grado $\le n\}$ é un sottospazio di $V$ che ha base $\{1,x,x^2,\dots,x^n\}$ con $n+1$ elementi, quindi $dim(U_n) = n+1$.

\subsection{Forma cartesiana e parametrica}
Sia $U$ sottospazio vettoriale di dimensione $d$ in uno spazio vettoriale di dimensione $n (d \le n)$. Posso esprimere $U$ in 2 forme:
\begin{itemize}
	\item forma \textbf{cartesiana}: $U$ é identificato in $K^n$ da $n - d$ equazioni tra loro linearmente indipendenti
	\item forma \textbf{parametrica}: $U$ é espresso in funzione di $d$ parametri
\end{itemize} 
Ad esempio, $V = R^4, U = \{(x,y,z,w) \in R^4 : x = 2y, y = 3z, w = 0\} \mbox{ (forma cartesiana)}$, oppure $U = \{(6t,3t,t,0), t \in R\} \mbox{ (forma parametrica)}$. In questo caso, $U$ ha dimensione 1, siccome posso scegliere solamente 1 parametro: una volta scelto, gli altri ne derivano di conseguenza. Ogni equazione (linearmente indipendente), nella forma cartesisana, toglie 1 grado di libertá.\\\\

\subsubsection{Cambio di forma}
\begin{itemize}
	\item da \textbf{cartesiana} a \textbf{parametrica}: si usano le equazioni per espliticare $n - d$ coordinate in funzione delle altre.\\
	Ad esempio, dato $\{(x,y,z) \in R^3 : x + 2y - z = 0\}$, risolvo $z = x+2y$, quindi se $x=t,y=s \implies z=t+2s$, che in forma cartesiana diventa $\{(t,s,t+2s), t,s \in R\}$
	\item da \textbf{parametrica} a \textbf{cartesiana}: si risolve il sistema.\\
	Ad esempio, $\{(t,-2t,s,t+2s),t,s \in R$:
	\begin{equation}
		\begin{cases}
			x = t \\
			y = -2t \\
			z = s \\
			w = t+2s \\
		\end{cases}
		\begin{cases}
			x = t \\
			2x + y = 0 & \mbox{(2I + II)} \\
			z = s \\
			x + 2z - w = 0 & \mbox{(I + 2III - IV)} \\	
		\end{cases}
	\end{equation}
	$ = \{(x,y,z,w) \in R^4 : 2x + y = 0 \wedge x + 2z - w = 0\}$
\end{itemize}

\subsection{Intersezione e unione di sottospazi vettoriali}
Osservazione. La forma cartesiana facilita l'\underline{intersezione dei sottospazi vettoriali}, mentre la forma parametrica risulta piú comoda per \underline{trovare le basi} di uno spazio.\\
Ad esempio, $U = \{(t,-2t,s,t+2s),t,s \in R\} = \{(t,-2t,0,t) + (0,0,s,2s),t,s \in R\} = \{t(1,-2,0,1) + s(0,0,1,2),t,s \in R\}$. $v_t = (1,-2,0,1), v_s = (0,0,1,2)$.\\
$\{tv_t + sv_s,t,s \in R\} = \langle v_t, v_t \rangle \implies$ $v_t,v_s$ generano $U$ e sono linearmente indipendenti $\implies v_t, v_s$ sono una base di $U$.\\\\
Proposizione. L'intersezione di sottospazi vettoriali é anch'esso un sottospazio vettoriale.\\
Dimostrazione. Sia $V$ uno spazio vettoriale su un campo $K$ e siano $U,W$ sottospazi vettoriali di $V$. $U \cap W = \{v \in V : v \in U, v \in W\}$.\\
Vogliamo mostrare che se $\forall v_1, v_2 \in U \cap W, v_1 + v_2 \in U \cap W$: in effetti, sapendo che $v_1, v_2 \in U, v_1, v_2 \in W$ e che $U$ e $W$ sono dei sottospazi, $v_1 + v_2 \in U, v_1 + v_2 \in W \implies v_1 + v_2 \in U \cap W$.\\\\
Al contrario, l'unione di sottospazi non sempre é un sottospazio. Ad esempio, in $V = R^2$, $U = \{(x,y) \in R^2 : x = 0\} = \{(0,t), t \in R\}, W = \{(x,y) \in R^2 : y = 0\} = \{(x,y) \in R^2 : y = 0\} = \{s,0), s \in R\}$.\\
$e_1 = (1,0) \in W, e_2 = (0,1) \in U \implies e_1, e_2 \in U \cup W$ ma $e_1 + e_2 = (1,1) \notin U, \notin W \implies e_1 + e_2 \notin U \cup W$.

\section{Somma di sottospazi}
Dato uno spazio vettoriale $V$ su un campo $K$, dati due sottospazi vettoriali $U$, $W$, la \textbf{somma} dei sottospazi $U$ e $W$ é il sottospazio $$U + W = \{u+w, u \in U, w \in W\}$$
$U + W$ é un sottospazio di $V$ perché $(u+w) + (u\prime + w\prime) = \underbrace{u + u\prime}_{\in U} + \underbrace{w + w\prime}_{\in W}$ ($\in U + W$). Analogamente, $\forall a \in K, a(u+w) = \underbrace{au}_{\in U} + \underbrace{aw}_{\in W}$ ($\in U + W$).

\subsection{Formula di Grassman}
Siano $U$, $W$ sottospazi vettoriali di $V$:
$$dim(U + W) = dim(U) + dim(W) - dim(U \cap W)$$
Intuitivamente, questa formula ricorda il modo di contare gli elementi appartenenti a due insiemi $A$ e $B$: elementi in $A$ + elementi in $B$ - elementi in $A \cap B$ siccome li ho giá contati negli elementi di $A$.\\\\
Idea alla base della dimostrazione:\\
Scelgo una base $v_1, \dots, v_k$ di $U \cap W$ ($dim(U \cap W) = k$) e la completo a una base $v_1, \dots, v_k ,u_1, \dots, u_m$ di $U$ ($dim(U) = k + m$) e a una base $v_1, \dots, v_k ,w_1, \dots, w_l$ di $W$ ($dim(W) = k + l$).\\
Si puó dimostrare che $v_1, \dots, v_k, u_1, \dots, u_m, w_1, \dots, w_l$ é una base di $U + W$, quindi $dim(U + W) = k + l + m = dim(U) + dim(W) - dim(U \cap W) = (k + l) + (k + m) - k$.

\subsection{Somma diretta}
Sia $U$, $W$ sottospazi vettoriali di $V$, allora $U$ e $W$ formano \textbf{somma diretta} se $U \cap W = \{\vec{0}\}$. Scriviamo $U \oplus W$ al posto di $U + W$.

\subsubsection{Implicazioni}
Sia $V$ spazio vettoriale, siano $U$, $W$ sottospazi vettoriali di $V$ tali che $V = U \oplus W$, allora $$\forall v \in V, \exists! u \in U, w \in W : v = u + w$$
ovvero ogni vettore $v$ di $V$ si scrive in modo unico come somma di un vettore $u \in U$ e $w \in W$.\\\\
Dimostrazione. Sappiamo che:
\begin{enumerate}
	\item $V = U + W$
	\item $U \cap W = \{\vec{0}\}$
\end{enumerate}
Per la 1, ogni vettore $v$ di $V$ si scrive come somma di un vettore $u \in U$ e $w \in W$ ($\forall v \in V, \exists u \in U, w \in W : v = u + w$).\\
Per mostrarne l'unicitá, supponiamo si possa scrivere anche come somma di altri due vettori $u\prime + w\prime, u\prime \in U, w\prime \in W$.\\
Quindi, $v = u + w = u\prime + w\prime \iff \underbrace{u - u\prime}_{\in U} = \underbrace{w\prime - w}_{\in W} \implies \in U \cap W$. Per la 2, $U \cap W = \{\vec{0}\}$, quindi $u - u\prime = \vec{0} \iff u = u\prime, w\prime - w = \vec{0} \iff w = w\prime$.

\subsubsection{Esempio}
$R^2 = U \oplus W$, $U = \{(x,y) \in R^2 : y = 0\}$, $W = \{(x,y) \in R^2 : x = 0\}$ (assi cartesiani): ogni vettore in $R^2$ si scrive in modo unico come somma di $u \in U, w \in W$.

\section{Applicazioni lineari}
Siano $U$, $V$ spazi vettoriali su $K$. Un'applicazione lineare $f : V \to U$ é \textbf{lineare} se é compatibile con le operazioni di $V$ e $U$, ovvero se:
\begin{itemize}
	\item $\forall v_1, v_2 \in V, f(v_1 + v_2) = f(v_1) + f(v_2)$
	\item $\forall v \in V, \forall a \in K, f(av) = af(v)$
\end{itemize}
equivalentemente, $\forall v_1, v_2 \in V, \forall a_1, a_2 \in K, f(a_1v_1 + a_2v_2) = a_1f(v_1) + a_2f(v_2)$ (coimplica 1 e 2).

\subsection{Nucleo e immagine}
Sia $f : V \to U$ applicazione lineare:
\begin{itemize}
	\item[] \textbf{nucleo} di $f$: $\Ker{f} = \{v \in V : f(v) = \vec{0}\}$
	\item[] \textbf{immagine} di $f$: $\Ima{f} = \{u \in U : \exists v \in V : u = f(v)\}$
\end{itemize}

\subsubsection{Nucleo e immagine sottospazi vettoriali}
\begin{enumerate}
	\item $\Ker$ é un sottospazio vettoriale di $V$
	\item $\Ima$ é un sottospazio vettoriale di $U$
\end{enumerate}
Dimostrazione:
\begin{enumerate}
	\item siano $v_1, v_2 \in \Ker{f}$ ($f(v_1) = f(v_2) = \vec{0}$), dobbiamo mostrare che anche $v_1 + v_2 \Ker{f}$:\\
	$f(v_1 + v_2) = f(v_1) + f(v_2) = \vec{0} + \vec{0} = \vec{0}$ (ragionamento analogo per lo scalare)
	\item siano $u_1, u_2 \in \Ima{f}$ ($\exists v_1, v_2 \in V : u_1 = f(v_1), u_2 = f(v_2)$), devo mostrare che anche $u_1 + u_2 \in \Ima{f}$:\\
	$f(v_1 + v_2) = f(v_1) + f(v_2) = u_1 + u_2 \in \Ima{f}$, ovvero $\exists v_1 + v_2 \in V : u_1 + u_2 = f(v_1 + v_2)$ (ragionamento analogo per lo scalare)
\end{enumerate}

\subsection{Iniettivitá e suriettivitá}
Sia $f : V \to U$ applicazione lineare:
\begin{itemize}
	\item $f$ é \textbf{suriettiva} $\iff$ $\Ima{f} = U$ ($\forall u \in U, \exists v \in V : u = f(v)$)
	\item $f$ é \textbf{iniettiva} $\iff$ $\Ker{f} = \{\vec{0}\}$ ($\forall v_1, v_2, v_1 \ne v_2, f(v_1) \ne f(v_2)$)
\end{itemize}

\subsubsection{Dimostrazione dell'iniettivitá}
\begin{itemize}
	\item[1 $\implies$ 2] Poiché $f$ é lineare, $f(\vec{0}) = f(v-v) = f(v) - f(v) = \vec{0}$ (il vettore nullo viene mandato nel vettore nullo). Siccome $f$ é iniettiva, se $v \ne \vec{0}, f(v) \ne f(\vec{0})$ (solamente il vettore nullo viene mandato nel vettore nullo), quindi $\Ker{f} = \{\vec{0}\}$
	\item 2 $\implies 1$ Siano $v_1, v_2 \in V : f(v_1) = f(v_2)$, devo mostrare che $v_1 = v_2$. Dalla 2, $f(v_1 - v_2) = f(v_1) - f(v_2) = \vec{0}$, ovvero $v_1 - v_2 \in \Ker{f}$, ma $\Ker{f} = \{\vec{0}\}$, quindi $v_1 - v_2 = \vec{0} \iff v_1 = v_2$, ovvero $f$ é iniettiva
\end{itemize}

\subsection{Teorema del rango}
Sia $f : V \to U$ applicazione lineare:
$$dim(V) = dim(\Ker{f}) + dim(\Ima{f})$$

\subsubsection{Dimostrazione}
Sia $v_1, \dots, v_k$ base di $\Ker{f}$ ($dim(\Ker{f}) = k$), la completo a una base $v_1, \dots, v_k, v_{k+1}, \dots, v_n$ di $V$ ($dim(V) = n$).\\
Avendo una base di $V$, so che $\forall v \in V, \exists! a_1, \dots, a_n \in K : v = a_1v_1 + \dots + a_nv_n$.\\
Poiché $f$ é lineare, $f(a_1v_1 + \dots + a_nv_n) = \underbrace{a_1v_1 + \dots a_kv_k}_{\in \Ker{f}} + a_{k+1}v_{k+1} + \dots + a_nv_n$.\\
Questo significa che ogni vettore $v$ di $V$ si scrive come combinazione lineare di $v_{k+1}, \dots, v_n$, ovvero $\{v_{k+1}, \dots, v_n\}$ é una base di $\Ima{f}$ ($dim(\Ima{f} = n - k$).\\
Infine, unendo i pezzi, ottengo $dim(V) = n = dim(\Ker{f}) + dim(\Ima{f}) = k + n - k$.

\subsection{Rapporto tra dimensione e iniettivitá/suriettivitá}
\begin{enumerate}
	\item se $dim(V) > dim(U)$, allora $f$ \textbf{non puó essere iniettiva}
	\item se $dim(V) < dim(U)$, allora $f$ \textbf{non puó essere suriettiva}
	\item se $dim(V) = dim(U)$, allora $f$ iniettiva $\iff$ $f$ suriettiva 
\end{enumerate}

\subsubsection{Dimostrazione}
\begin{enumerate}
	\item $dim(\Ima{f}) \le dim(U) < dim(V) \underbrace{=}_{\mbox{teorema del rango}} dim(\Ima{f}) + dim(\Ker{f}) \iff dim(\Ima{f}) < dim(\Ima{f}) + dim(\Ker{f}) \iff dim(\Ker{f})> 0 \implies \Ker{f} \ne \{\vec{0}\}$ ($f$ non é iniettiva)
	\item $dim(\Ima{f}) + dim(\Ker{f}) < dim(U) \implies dim(\Ima{f}) \ne dim(U)$ ($dim(\Ker{f}) \ge 0$), quindi $f$ non é suriettiva perché non é $=U$
	\item $dim(\Ker{f}) + dim(\Ima{f}) = dim(V) = dim(U)$. Se $f$ é iniettiva, $dim(\Ker{f}) = 0 \iff dim(\Ima{f}) = dim(U)$, ovvero $f$ é suriettiva
\end{enumerate}

\end{document}